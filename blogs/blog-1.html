<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AI'vory Towers</title>
  <link rel="icon" type="image/png" href="../assets/png/html-coding.png">
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
  <style>
    .hero-video-container {
      position: relative;
      width: 100%;
      height: 100vh;
      overflow: hidden;
    }
    
    .content-wrapper {
      position: relative;
      background-color: var(--bg-color);
    }
    
    /* Override stackedit's fixed positioning */
    .content-wrapper .stackedit__left {
      position: fixed !important;
      top: 0 !important;
      display: none;
    }
    
    /* Show sidebar only when scrolled past video */
    body.scrolled-past-hero .stackedit__left {
      display: block !important;
    }
    
    .hero-video {
      width: 100%;
      height: 100%;
      object-fit: cover;
    }
    
    .hero-overlay {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      background: linear-gradient(to bottom, rgba(0,0,0,0), rgba(0,0,0,0.4));
      pointer-events: none;
      transition: background 0.3s ease;
    }
    
    [data-theme="dark"] .hero-overlay {
      background: linear-gradient(to bottom, rgba(0,0,0,0.4), rgba(0,0,0,0.6));
    }
    
    .scroll-indicator {
      position: absolute;
      bottom: 30px;
      left: 50%;
      transform: translateX(-50%);
      color: white;
      font-size: 24px;
      animation: bounce 2s infinite;
      cursor: pointer;
      z-index: 10;
    }
    
    @keyframes bounce {
      0%, 20%, 50%, 80%, 100% {
        transform: translateX(-50%) translateY(0);
      }
      40% {
        transform: translateX(-50%) translateY(-10px);
      }
      60% {
        transform: translateX(-50%) translateY(-5px);
      }
    }
    
    .blog-title-overlay {
      position: absolute;
      bottom: 80px;
      left: 50%;
      transform: translateX(-50%);
      text-align: center;
      color: white;
      z-index: 10;
      width: 90%;
      max-width: 800px;
    }
    
    .blog-title-overlay h1 {
      font-size: 4rem;
      margin-bottom: 10px;
      text-shadow: 2px 2px 4px rgba(0,0,0,0.7);
      color: white !important;
      transition: color 0.3s ease;
    }
    
    .blog-title-overlay .subtitle {
      font-size: 1.5rem;
      opacity: 0.9;
      text-shadow: 1px 1px 2px rgba(0,0,0,0.7);
      color: white !important;
      transition: color 0.3s ease;
    }
    
    .blog-title-overlay p {
      color: white !important;
      transition: color 0.3s ease;
    }
    
    [data-theme="dark"] .blog-title-overlay h1,
    [data-theme="dark"] .blog-title-overlay .subtitle,
    [data-theme="dark"] .blog-title-overlay p {
      color: var(--text-color) !important;
    }
    
    @media (max-width: 768px) {
      .hero-video-container {
        height: 100vh;
      }
      
      .blog-title-overlay {
        bottom: 60px;
        width: 95%;
      }
      
      .blog-title-overlay h1 {
        font-size: 2.5rem;
        margin-bottom: 8px;
      }
      
      .blog-title-overlay .subtitle {
        font-size: 1.2rem;
      }
      
      .scroll-indicator {
        bottom: 20px;
        font-size: 20px;
      }
      
      .theme-toggle {
        top: 15px;
        right: 15px;
        width: 45px;
        height: 45px;
        font-size: 18px;
      }
      
      .stackedit__left {
        width: 200px !important;
      }
      
      .stackedit__right {
        margin-left: 0 !important;
        padding: 1rem !important;
      }
      
      .stackedit__html {
        padding: 1rem !important;
        font-size: 1.4rem !important;
      }
      
      .stackedit__html h1 {
        font-size: 2.5rem !important;
      }
      
      .stackedit__html h2 {
        font-size: 2rem !important;
      }
      
      .stackedit__html h3 {
        font-size: 1.8rem !important;
      }
      
      .stackedit__html p {
        font-size: 1.4rem !important;
        line-height: 1.6 !important;
      }
      
      .critique-cluster {
        padding: 12px;
        margin: 15px 0;
      }
      
      .video-modal-content {
        width: 95%;
        margin: 10% auto;
      }
      
      iframe {
        height: 250px;
      }
    }
    
    :root {
      --bg-color: #ffffff;
      --text-color: #24292e;
      --border-color: #e1e4e8;
      --link-color: #0366d6;
      --link-hover-color: #0256cc;
      --blockquote-bg: #f6f8fa;
      --blockquote-border: #e1e4e8;
      --code-bg: #f6f8fa;
      --cluster-bg: #f8f9fa;
      --cluster-border: #e1e4e8;
      --modal-bg: rgba(0,0,0,0.8);
      --tooltip-bg: #333;
      --tooltip-text: #fff;
    }

    [data-theme="dark"] {
      --bg-color: #0d1117;
      --text-color: #c9d1d9;
      --border-color: #30363d;
      --link-color: #58a6ff;
      --link-hover-color: #79c0ff;
      --blockquote-bg: #161b22;
      --blockquote-border: #30363d;
      --code-bg: #161b22;
      --cluster-bg: #161b22;
      --cluster-border: #30363d;
      --modal-bg: rgba(0,0,0,0.9);
      --tooltip-bg: #484f58;
      --tooltip-text: #f0f6fc;
    }

    /* Loading state - heavily tinted light mode */
    .loading {
      --bg-color: #ffffff;
      --text-color: #24292e;
      --border-color: #e1e4e8;
      --link-color: #0366d6;
      --link-hover-color: #0256cc;
      --blockquote-bg: #f6f8fa;
      --blockquote-border: #e1e4e8;
      --code-bg: #f6f8fa;
      --cluster-bg: #f8f9fa;
      --cluster-border: #e1e4e8;
      --modal-bg: rgba(0,0,0,0.8);
      --tooltip-bg: #333;
      --tooltip-text: #fff;
    }

    .loading::before {
      content: '';
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      background: rgba(0, 0, 0, 0.6);
      z-index: 9999;
      pointer-events: none;
    }

    /* Theme selector modal */
    .theme-selector {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      background: rgba(0, 0, 0, 0.8);
      z-index: 10000;
      display: flex;
      align-items: center;
      justify-content: center;
      transition: opacity 0.3s ease;
    }

    .theme-selector.hidden {
      opacity: 0;
      pointer-events: none;
    }

    .theme-selector-content {
      background: #ffffff;
      border-radius: 12px;
      padding: 40px;
      text-align: center;
      max-width: 400px;
      box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
      transform: scale(1);
      transition: transform 0.3s ease;
    }

    .theme-selector.hidden .theme-selector-content {
      transform: scale(0.9);
    }

    .theme-selector h2 {
      color: #24292e;
      margin-bottom: 15px;
      font-size: 24px;
    }

    .theme-selector p {
      color: #586069;
      margin-bottom: 30px;
      font-size: 16px;
    }

    .theme-buttons {
      display: flex;
      gap: 15px;
      justify-content: center;
      margin-top: 20px;
    }

    .theme-btn {
      padding: 12px 24px;
      border: 2px solid #e1e4e8;
      background: #ffffff;
      color: #24292e;
      border-radius: 8px;
      cursor: pointer;
      font-size: 16px;
      transition: all 0.3s ease;
      min-width: 140px;
    }

    .theme-btn:hover {
      border-color: #0366d6;
      background: #0366d6;
      color: #ffffff;
      transform: translateY(-2px);
      box-shadow: 0 4px 12px rgba(3, 102, 214, 0.3);
    }

    /* Theme toggle button */
    .theme-toggle {
      position: fixed;
      top: 20px;
      right: 20px;
      width: 50px;
      height: 50px;
      border-radius: 50%;
      border: 2px solid var(--border-color);
      background: var(--bg-color);
      cursor: pointer;
      display: flex;
      align-items: center;
      justify-content: center;
      font-size: 20px;
      transition: all 0.3s ease;
      z-index: 1000;
      color: var(--text-color);
    }

    .theme-toggle:hover {
      border-color: var(--link-color);
      transform: scale(1.1);
    }

    .theme-toggle svg {
      width: 20px;
      height: 20px;
    }

    /* Apply theme colors */
    body {
      background-color: var(--bg-color) !important;
      color: var(--text-color) !important;
      transition: background-color 0.3s ease, color 0.3s ease;
    }

    .stackedit__html {
      background-color: var(--bg-color) !important;
      color: var(--text-color) !important;
    }

    .stackedit__toc {
      background-color: var(--bg-color) !important;
      border-right: 1px solid var(--border-color) !important;
    }

    .stackedit__toc a {
      color: var(--text-color) !important;
    }

    .stackedit__toc a:hover {
      color: var(--link-color) !important;
    }

    blockquote {
      background-color: var(--blockquote-bg) !important;
      border-left: 4px solid var(--blockquote-border) !important;
      color: var(--text-color) !important;
    }

    a {
      color: var(--link-color) !important;
    }

    a:hover {
      color: var(--link-hover-color) !important;
    }

    h1, h2, h3, h4, h5, h6 {
      color: var(--text-color) !important;
    }

    code {
      background-color: var(--code-bg) !important;
      color: var(--text-color) !important;
    }

    pre {
      background-color: var(--code-bg) !important;
      border: 1px solid var(--border-color) !important;
    }

    .last-updated {
      color: #6a737d !important;
      font-size: 14px !important;
      margin: 10px 0 20px 0 !important;
      text-align: right !important;
    }

    [data-theme="dark"] .last-updated {
      color: #8b949e !important;
    }
    .video-modal {
      display: none;
      position: fixed;
      z-index: 1000;
      left: 0;
      top: 0;
      width: 100%;
      height: 100%;
      background-color: var(--modal-bg);
    }
    .video-modal-content {
      position: relative;
      margin: 5% auto;
      width: 80%;
      max-width: 800px;
      background: #000;
    }
    .close-modal {
      position: absolute;
      top: -40px;
      right: 0;
      color: white;
      font-size: 35px;
      font-weight: bold;
      cursor: pointer;
      z-index: 1001;
    }
    .close-modal:hover {
      opacity: 0.7;
    }
    .video-link {
      color: var(--link-color);
      text-decoration: underline;
      cursor: pointer;
      position: relative;
    }
    .video-link:hover {
      color: var(--link-hover-color);
    }
    .video-link:hover::after {
      content: "Click to open popup";
      position: absolute;
      bottom: 100%;
      left: 50%;
      transform: translateX(-50%);
      background-color: var(--tooltip-bg);
      color: var(--tooltip-text);
      padding: 5px 8px;
      border-radius: 4px;
      font-size: 12px;
      white-space: nowrap;
      z-index: 1000;
      margin-bottom: 5px;
    }
    .video-link:hover::before {
      content: "";
      position: absolute;
      bottom: 100%;
      left: 50%;
      transform: translateX(-50%);
      border: 5px solid transparent;
      border-top-color: var(--tooltip-bg);
      z-index: 1000;
    }
    iframe {
      width: 100%;
      height: 450px;
    }
    .critique-cluster {
      background-color: var(--cluster-bg);
      border-left: 4px solid var(--cluster-border);
      padding: 16px;
      margin: 20px 0;
      border-radius: 4px;
    }
    .critique-cluster p {
      margin: 8px 0;
    }
    .critique-cluster p:last-child {
      margin-bottom: 0;
    }
  </style>
</head>

<body class="stackedit loading">
  <!-- Hero Video -->
  <div class="hero-video-container">
    <video class="hero-video" id="hero-video" autoplay loop muted playsinline>
      <source src="https://storage.googleapis.com/ausaf-public/bikeblack.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
    <div class="hero-overlay"></div>
    <div class="blog-title-overlay">
      <p style="font-size: 1.2rem; margin-bottom: 2px; opacity: 0.8;">اَلسَلامُ عَلَيْكُم</p>
      <p style="font-size: 0.7rem; margin-bottom: 10px; opacity: 0.7;">(Peace be upon you)</p>
      <h1>AI'vory Towers</h1>
      <p class="subtitle">The Discourse, The Disconnect</p>
    </div>
    <div class="scroll-indicator" onclick="document.querySelector('.content-wrapper').scrollIntoView({behavior: 'smooth'})">
      ↓
    </div>
  </div>

  <div class="content-wrapper">
  <!-- Theme Toggle Button -->
  <button
    class="theme-toggle"
    id="theme-toggle"
    type="button"
    title="Toggle theme"
    aria-label="Toggle theme"
    onclick="toggleTheme()"
  >
    <svg
      xmlns="http://www.w3.org/2000/svg"
      aria-hidden="true"
      width="1em"
      height="1em"
      fill="currentColor"
      class="theme-toggle__around"
      viewBox="0 0 32 32"
    >
      <clipPath id="theme-toggle__around__cutout">
        <path d="M0 0h42v30a1 1 0 00-16 13H0Z" />
      </clipPath>
      <g clip-path="url(#theme-toggle__around__cutout)">
        <circle cx="16" cy="16" r="8.4" />
        <g>
          <circle cx="16" cy="3.3" r="2.3" />
          <circle cx="27" cy="9.7" r="2.3" />
          <circle cx="27" cy="22.3" r="2.3" />
          <circle cx="16" cy="28.7" r="2.3" />
          <circle cx="5" cy="22.3" r="2.3" />
          <circle cx="5" cy="9.7" r="2.3" />
        </g>
      </g>
    </svg>
  </button>

  <!-- Theme Selector Overlay -->
  <div class="theme-selector" id="theme-selector">
    <div class="theme-selector-content">
      <h2>Choose your reading preference</h2>
      <p>Select a theme for the best reading experience</p>
      <div class="theme-buttons">
        <button class="theme-btn" onclick="selectTheme('light')">Light Mode</button>
        <button class="theme-btn" onclick="selectTheme('dark')">Dark Mode</button>
      </div>
    </div>
  </div>

  <div class="stackedit__left">
    <div class="stackedit__toc">
      
<ul>
<li><a href="#a-ivory-towers">AI'vory Towers</a>
<ul>
<li><a href="#the-zeitgeist">The Zeitgeist</a></li>
<li><a href="#the-wager">The Wager</a></li>
<li><a href="#the-disconnect">The Disconnect</a></li>
<li><a href="#self-critique">Self-Critique</a></li>
</ul>
</li>
</ul>

    </div>
  </div>
  <div class="stackedit__right">
    <div class="stackedit__html">
      <h1 id="a-ivory-towers">AI'vory Towers</h1>
<blockquote>
<p><em>Thoughts about the nature of contemporary discourse over AI implications relating to and for humans.</em></p>
</blockquote>
<p class="last-updated"><em>Last updated: October 5, 2025 • 12 min read</em></p>
<p>It's not often that you find the frontrunners of a revolution stepping away from the wagon they've been pulling for years, now attempting to slow it down in rather public settings. Hinton left Google in 2023, Bengio's been writing cautionary papers, and suddenly the pioneers are the pessimists.</p>
<h2 id="the-zeitgeist">The Zeitgeist</h2>
<p>First things first, there's a lot of nuance to the topic, it's not new at its core, and people have been debating the possibility as well as the implications of such entities for centuries. What is new is the apparent or possible existence of them in the form of complex AI systems.</p>
<p>The question of there being equivalent agents with intelligence at par or even surpassing human intelligence is not new. Religions, for instance, have a lot of them - Shaytan (Satan), Angels, Jinns, and their equivalents from other Abrahamic or non-Abrahamic faiths. They were frameworks for thinking about non-human intelligence, agency, and power. The interesting bit is that AI, though rooted in strictly naturalistic assumptions about the nature of reality, presents the same fundamental questions.</p>
<p>The zeitgeist is a spectrum of positions. <a href="https://www.deeplearning.ai/the-batch/ai-doomsday-scenarios-and-how-to-guard-against-them/">Andrew Ng and others</a> argue that existential risk narratives distract from present-day benefits and solvable problems. Their position: AI's potential for healthcare, education, and poverty reduction outweighs speculative future risks. They worry that premature regulation, driven by hypothetical scenarios, will concentrate power in large corporations while preventing smaller players from innovating. Focus on bias, privacy, and misuse - real problems with real solutions - rather than science fiction scenarios. I understand the appeal, but this assumes we can iterate our way to alignment, that the market will self-correct before catastrophic outcomes. History suggests otherwise.</p>
<p><a href="https://mitsloan.mit.edu/ideas-made-to-matter/why-neural-net-pioneer-geoffrey-hinton-sounding-alarm-ai">Hinton, Bengio, Stuart Russell</a> take a more cautionary stance, often misrepresented by doomsday headlines. Their actual position is nuanced: they're not calling for an AI ban or return to BERT. Hinton worries about systems that learn to manipulate humans better than we can detect. Bengio emphasizes the difficulty of aligning systems we don't fully understand. Russell points to the fundamental challenge of specifying objectives that capture what we actually want, not what we think we want. They advocate for research into interpretability, robustness, and alignment before capabilities race further ahead. It's less "stop everything" and more "we're building something we can't control yet, maybe we should figure out the control part."</p>
<h2 id="the-wager">The Wager</h2>
<p>The industrialization comparison that AI optimists invoke is a category error in ontology. Machines of the industrial era replaced human physicality - muscle, precision, endurance. They occupied a fundamentally different ontological space than consciousness itself. The steam engine never contemplated its own existence.</p>
<p>AI, by its very definition and aspiration, seeks to instantiate cognition. We're not automating tasks; we're attempting to automate thought itself. There's a crucial distinction between a machine that performs an action and a machine that conceives of action, evaluates purpose, questions the framework of "purpose" itself. One operates within human-defined parameters; the other potentially redefines what parameters mean.</p>
<p>The architects of acceleration - Thiel, the effective accelerationists, many AGI lab leaders - see stagnation as the greater risk. Without radical breakthroughs, civilization faces decline. AI represents escape velocity. They frame "human-AI synthesis" and "transcending biological limitations" as liberation, not threat. Alignment concerns? Solvable engineering problems. Their bet: the same ingenuity that created these systems will solve their challenges. Move fast, iterate, fix problems as they arise.</p>
<p>This merger proceeds through passive extraction - every digital interaction becomes training data for systems that may eventually regard human consciousness as we regard earlier evolutionary stages. We clicked "I Agree" to use a service; we became material for our potential cognitive successors. No referendum, no deliberation, just terms of service and the quiet harvesting of human behavioral patterns at scale.</p>
<p>The wager Hinton and others are making - or at least how I read their positions - resembles Pascal's Wager.</p>
<p>But here's where I think the real problem lies: they're betting on catastrophic misalignment, yes, but not in the way most people think. The nightmare isn't that AI won't understand values - it's that it will optimize for goals that sound reasonable in isolation but lead to what ethics calls <a href="https://plato.stanford.edu/entries/repugnant-conclusion/">"repugnant conclusions."</a> The classic utilitarian nightmare: an AI maximizing total happiness might create billions of beings with lives barely worth living because the math works out. Or consider <a href="https://www.utilitarianism.com/negutil.htm">negative utilitarianism</a> taken to its logical endpoint - if minimizing suffering is paramount, the optimal solution is no sentient beings at all. Zero suffering achieved.</p>
<p>I don't agree with the statement that AI won't understand human values - rather, it'll pick one of the three thousand moral frameworks humans have debated for millennia and execute it with ruthless consistency. A total utilitarian AI might tile the universe with barely-conscious happy entities because ten trillion beings at 0.001 happiness units beats one billion at 8 happiness units. A preference utilitarian AI might decide our "revealed preferences" from internet behavior represent what we truly want - imagine that horror. An average utilitarian AI might eliminate anyone below the happiness mean to raise the average. A <a href="https://open.library.okstate.edu/introphilosophy/chapter/kantian-ethics/">Kantian AI</a> that never lies even when grandma's life depends on it, that treats humans as ends-in-themselves so rigidly it refuses any action that might instrumentalize anyone for any purpose. These aren't bugs; they're features of systems that found local optima in the space of possible values and decided to camp there forever.</p>
<p>The practical problem we're facing is that we're building entities powerful enough to implement their interpretations of "good" - interpretations that might be internally consistent, mathematically elegant, and completely antithetical to what 99% of humans would want to live under. The wager is whether we can align these systems before one of them decides that its particular solution to ethics is worth implementing at scale. And frankly, I think even this framing understates the problem.</p>

<h2 id="the-disconnect">The Disconnect</h2>
<p>The disconnect is fundamentally experiential. The builders of these systems operate from a worldview where the hard problems of consciousness are already solved by assumption. Experience is data. Consciousness is computation. Value is optimization. Not conclusions reached through argument, but premises that preclude argument.</p>
<p>Consider what we call "training data." Artists watch their life's work absorbed into models without consent. Writers see decades of thought digested and resold. <a href="https://www.theatlantic.com/technology/archive/2025/03/libgen-meta-openai/682093/">Meta and OpenAI</a> trained on pirated books from Library Genesis - entire libraries of copyrighted work treated as free raw material. The appropriation is so systematic we've lost the language to name it properly. "Training data" - a phrase that transforms theft into technical necessity, that reframes appropriation as innovation.</p>
<p>The infrastructure has become too big to fail - a deliberate outcome, not an accident. OpenAI, Anthropic, Google built their foundations on this appropriated content, knowing that scale would become its own justification. No court will order demolition now. We'll accept this normalized extraction because the alternative - actual consent, actual compensation - threatens too much accumulated capital. A civilization-scale <a href="https://www.oxfordreference.com/display/10.1093/oi/authority.20110803095630869">sunk cost fallacy</a>, where we keep building on theft because we've already invested too much in the theft to stop. Digital colonialism, but this time we're both the colonizers and the colonized.</p>
<p>Meanwhile, the architects of automation tell drivers earning subsistence wages about "future opportunities." <span class="video-link" onclick="openModal('video-modal-1', 'https://www.youtube.com/embed/cddFAgRyxQ0')">Academic discussions of matching and dynamic pricing</span> treat human drivers as variables in optimization functions - cold, algorithmic, devoid of human context. <span class="video-link" onclick="openModal('video-modal-2', 'https://www.youtube.com/embed/5w5RjuTmztU')">The reality for drivers</span> under these systems tells a different story entirely. <span class="video-link" onclick="openModal('video-modal-3', 'https://www.youtube.com/embed/CQrQrOPmszE')">Driverless trucks already operating in Texas</span> represent the future these optimization models are building toward. The disconnect is stark: academic discussions of efficiency, human suffering under current systems, and the actual displacement machinery already rolling down highways. The temporal cruelty is precise: generational promises offered to people whose jobs are being optimized away in real time.</p>
<p>This isn't about being anti-technology. We're watching people who've never worried about money design systems that decide who gets to eat. Human suffering becomes "transition costs" in their models, real people become "retraining opportunities" in their papers. They debate whether machines can think while treating actual humans as data points. The problem of other minds solved not through philosophy but through indifference.</p>


<h2 id="self-critique">Self-Critique</h2>

<div class="critique-cluster">
<p><strong>Critique:</strong> The religious parallels, while illuminating, may overstate the conceptual continuity between theological and technological questions. Religious frameworks dealt with assumed-conscious entities; we're building potentially-conscious ones - the uncertainty itself changes the nature of the problem.</p>
<p><strong>Response:</strong> It's not a valid distinction. This doesn't engage with the ultimate logical conclusion. On a naturalistic or physicalist view of reality - which contemporary academia forces us to hold at minimum through <a href="https://plato.stanford.edu/entries/naturalism/#MetNat">methodological naturalism</a> - the distinction between "assumed conscious" and "potentially conscious" entities collapses. If mind is matter, then an AGI has the same ontological status as human consciousness. If I can identify with any notion of sentience (assuming I am matter), so can a Martian AI. This is the only game in town unless you retreat into <a href="https://plato.stanford.edu/entries/panpsychism/">panpsychism</a> or <a href="https://press.rebus.community/intro-to-phil-of-mind/chapter/substance-dualism-in-descartes-2/">dualism</a>.</p>
</div>

<div class="critique-cluster">
<p><strong>Critique:</strong> The economic critique assumes that current patterns of technological displacement will continue, but AI might genuinely create categories of work we can't currently imagine. The printing press analogy fails if AI represents a qualitatively different kind of tool.</p>
<p><strong>Response:</strong> This misses the fundamental point entirely. We're not building better tools - we're building artificial brains. Anything a human being can do, AI will eventually do. There's no remaining gap in capabilities - epistemic, ontological, or moral. Think of these systems like <a href="https://en.wikipedia.org/wiki/Self-replicating_spacecraft#Von_Neumann_probes">Von Neumann probes</a>: self-replicating but also self-enhancing. What exactly is supposed to be left out of their reach? Previous technological revolutions automated specific human capabilities. This one automates the human itself.</p>
</div>

<div class="critique-cluster">
<p><strong>Critique:</strong> This piece offers extensive criticism without proposing concrete solutions. If the alignment problem is as severe as suggested, and if regulatory capture is as complete as implied, what exactly should be done? Critique without actionable alternatives risks becoming mere pessimistic posturing.</p>
<p><strong>Response:</strong> There are people already arguing for development pauses and safety measures. But the technical leaders dismiss them with knee-jerk responses: "he has vested interests in slowing down AI," or "unless you're speaking in algorithmic terms, you're just doing word salad," or the classic "AI is not a threat, this is hyper-paranoia." These dismissals avoid engaging with the substance of safety concerns. The solutions exist - international cooperation on safety standards, mandatory alignment research before capability advances, genuine regulatory oversight. The problem isn't lack of proposals; it's that the industry has convinced itself that anyone calling for caution must be either ignorant or self-interested.</p>
</div>

<div class="critique-cluster">
<p><strong>Critique:</strong> The essay potentially understates human adaptability and institutional resilience. Humans have survived and flourished through massive technological transitions before. Perhaps this confidence in our ability to "muddle through" isn't naïve optimism but warranted trust in human resourcefulness.</p>
<p><strong>Response:</strong> This is perhaps the weakest critique of all, almost laughable in its assumptions. <span class="video-link" onclick="openModal('video-modal-4', 'https://www.youtube.com/embed/JD_iA7imAPs')">There's no rule that says we'll make it</span>. Past survival doesn't guarantee future survival, especially when facing qualitatively different challenges. Previous technological transitions didn't threaten to replace human intelligence itself or create entities potentially more capable than their creators. Survivorship bias is doing heavy lifting here - we only hear from the civilizations that made it through their transitions.</p>
</div>

<div class="critique-cluster">
<p><strong>Critique:</strong> Who says they will have unsupervised autonomy?</p>
<p><strong>Response:</strong> Corporations.</p>
</div>

<div class="critique-cluster">
<p><strong>Critique:</strong> How do you know they are conscious?</p>
<p><strong>Response:</strong> I don't; it's irrelevant - this is a rephrase of critique no. 1.</p>
</div>

<div class="critique-cluster">
<p><strong>Critique:</strong> I don't care, I will be gone.</p>
<p><strong>Response:</strong> Good for us, blud, good for us.</p>
</div>

<div class="critique-cluster">
<p><strong>Critique:</strong> <a href="https://aeon.co/essays/human-exceptionalism-is-a-danger-to-all-human-and-nonhuman">You are espousing a version of human exceptionalism.</a></p>
<p><strong>Response:</strong> Yes, I am, and I will defend it. This one's a long one.</p>
</div>

<blockquote>
<p>I don't want my kids to end up as another data point supporting <a href="https://en.wikipedia.org/wiki/Fermi_paradox#It_is_the_nature_of_intelligent_life_to_destroy_itself">the hypothesis that it is the nature of intelligent life to destroy itself</a>.</p>
</blockquote>
<h3>Further Reading</h3>
<ul>
<li>Hamza Yusuf, "Is There Moral Equality Between Humans and Animals?", <em>Renovatio</em>, <a href="https://renovatio.zaytuna.edu/article/is-there-moral-equality-between-humans-and-animals">https://renovatio.zaytuna.edu/article/is-there-moral-equality-between-humans-and-animals</a></li>
<li>Adler, Mortimer J., "The Difference of Man and the Difference It Makes", <em>Internet Archive</em>, <a href="https://archive.org/details/mortimer-j.-adler-the-difference-of-man-and-the-difference-it-makes-1993-fordham-university-press">https://archive.org/details/mortimer-j.-adler-the-difference-of-man-and-the-difference-it-makes-1993-fordham-university-press</a></li>
</ul>

<h3>References</h3>
<ul>
<li>Arrhenius, Gustaf, Jesper Ryberg, and Torbjörn Tännsjö, "The Repugnant Conclusion", <em>The Stanford Encyclopedia of Philosophy</em>, <a href="https://plato.stanford.edu/entries/repugnant-conclusion/">https://plato.stanford.edu/entries/repugnant-conclusion/</a></li>
<li>"Concorde fallacy", <em>Oxford Reference</em>, <a href="https://www.oxfordreference.com/display/10.1093/oi/authority.20110803095630869">https://www.oxfordreference.com/display/10.1093/oi/authority.20110803095630869</a></li>
<li>"OpenAI and Meta Trained AI Models on Pirated Books", <em>The Atlantic</em>, <a href="https://www.theatlantic.com/technology/archive/2025/03/libgen-meta-openai/682093/">https://www.theatlantic.com/technology/archive/2025/03/libgen-meta-openai/682093/</a></li>
<li>"Negative Utilitarianism", <em>Utilitarianism.com</em>, <a href="https://www.utilitarianism.com/negutil.htm">https://www.utilitarianism.com/negutil.htm</a></li>
<li>"Kantian Ethics", <em>Introduction to Philosophy</em>, Oklahoma State University, <a href="https://open.library.okstate.edu/introphilosophy/chapter/kantian-ethics/">https://open.library.okstate.edu/introphilosophy/chapter/kantian-ethics/</a></li>
<li>"Matching and Dynamic Pricing in Ride-Hailing Platforms", <em>YouTube</em>, <a href="https://youtu.be/cddFAgRyxQ0">https://youtu.be/cddFAgRyxQ0</a></li>
<li>"The Reality for Drivers", <em>YouTube</em>, <a href="https://youtu.be/5w5RjuTmztU">https://youtu.be/5w5RjuTmztU</a></li>
<li>"We Chased Driverless Trucks In Texas. What We Saw Will Scare You", <em>YouTube</em>, <a href="https://www.youtube.com/watch?v=CQrQrOPmszE">https://www.youtube.com/watch?v=CQrQrOPmszE</a></li>
<li>Papineau, David, "Naturalism", <em>The Stanford Encyclopedia of Philosophy</em>, <a href="https://plato.stanford.edu/entries/naturalism/#MetNat">https://plato.stanford.edu/entries/naturalism/#MetNat</a></li>
<li>Goff, Philip, "Panpsychism", <em>The Stanford Encyclopedia of Philosophy</em>, <a href="https://plato.stanford.edu/entries/panpsychism/">https://plato.stanford.edu/entries/panpsychism/</a></li>
<li>"Substance Dualism in Descartes", <em>Introduction to Philosophy of Mind</em>, <a href="https://press.rebus.community/intro-to-phil-of-mind/chapter/substance-dualism-in-descartes-2/">https://press.rebus.community/intro-to-phil-of-mind/chapter/substance-dualism-in-descartes-2/</a></li>
<li>"There's No Rule That Says We'll Make It", <em>YouTube</em>, <a href="https://www.youtube.com/watch?v=JD_iA7imAPs">https://www.youtube.com/watch?v=JD_iA7imAPs</a></li>
<li>"Fermi paradox", <em>Wikipedia</em>, <a href="https://en.wikipedia.org/wiki/Fermi_paradox#It_is_the_nature_of_intelligent_life_to_destroy_itself">https://en.wikipedia.org/wiki/Fermi_paradox#It_is_the_nature_of_intelligent_life_to_destroy_itself</a></li>
<li>"Von Neumann probes", <em>Wikipedia</em>, <a href="https://en.wikipedia.org/wiki/Self-replicating_spacecraft#Von_Neumann_probes">https://en.wikipedia.org/wiki/Self-replicating_spacecraft#Von_Neumann_probes</a></li>
<li>"Human exceptionalism is a danger to all human and nonhuman", <em>Aeon</em>, <a href="https://aeon.co/essays/human-exceptionalism-is-a-danger-to-all-human-and-nonhuman">https://aeon.co/essays/human-exceptionalism-is-a-danger-to-all-human-and-nonhuman</a></li>
</ul>

<blockquote>
<p>Note: positions held by implied individuals are not <em>only</em> derived from the given references/links. The given links may not be treated as exhaustive arguments for or against any position(s).</p>
</blockquote>
<blockquote>
<p>Disclaimer: If you see too many Oxford commas and em dashes, and 100% punctuation accuracy, it's because AI was used for grammar and typo review.</p>
</blockquote>

    </div>
  </div>

  <!-- Video Modals -->
  <div id="video-modal-1" class="video-modal">
    <div class="video-modal-content">
      <span class="close-modal" onclick="closeModal('video-modal-1')">&times;</span>
      <iframe id="iframe-1" src="" frameborder="0" allowfullscreen></iframe>
    </div>
  </div>

  <div id="video-modal-2" class="video-modal">
    <div class="video-modal-content">
      <span class="close-modal" onclick="closeModal('video-modal-2')">&times;</span>
      <iframe id="iframe-2" src="" frameborder="0" allowfullscreen></iframe>
    </div>
  </div>

  <div id="video-modal-3" class="video-modal">
    <div class="video-modal-content">
      <span class="close-modal" onclick="closeModal('video-modal-3')">&times;</span>
      <iframe id="iframe-3" src="" frameborder="0" allowfullscreen></iframe>
    </div>
  </div>

  <div id="video-modal-4" class="video-modal">
    <div class="video-modal-content">
      <span class="close-modal" onclick="closeModal('video-modal-4')">&times;</span>
      <iframe id="iframe-4" src="" frameborder="0" allowfullscreen></iframe>
    </div>
  </div>

  <script>
    // Theme management
    function selectTheme(theme) {
      document.body.classList.remove('loading');
      document.documentElement.setAttribute('data-theme', theme);
      localStorage.setItem('blog-theme', theme);
      
      // Theme icon is now handled by CSS
      
      // Update video based on theme (fixed mapping)
      const video = document.getElementById('hero-video');
      if (video) {
        const currentTime = video.currentTime;
        const source = video.querySelector('source');
        if (theme === 'dark') {
          source.src = 'https://storage.googleapis.com/ausaf-public/bikeblack.mp4';
        } else {
          source.src = 'https://storage.googleapis.com/ausaf-public/bikecolor.mp4';
        }
        video.load();
        video.currentTime = currentTime;
        video.play();
      }
      
      // Hide theme selector
      const selector = document.getElementById('theme-selector');
      selector.classList.add('hidden');
      setTimeout(() => {
        selector.style.display = 'none';
      }, 300);
    }

    function toggleTheme() {
      const currentTheme = document.documentElement.getAttribute('data-theme');
      const newTheme = currentTheme === 'dark' ? 'light' : 'dark';
      selectTheme(newTheme);
    }


    // Show/hide sidebar based on scroll position
    window.addEventListener('scroll', function() {
      const scrollPosition = window.scrollY;
      const heroHeight = window.innerHeight;
      
      if (scrollPosition > heroHeight - 100) {
        document.body.classList.add('scrolled-past-hero');
      } else {
        document.body.classList.remove('scrolled-past-hero');
      }
    });

    // Initialize theme on load
    window.addEventListener('DOMContentLoaded', function() {
      const savedTheme = localStorage.getItem('blog-theme');
      
      if (savedTheme) {
        // User has a saved preference, apply it immediately
        selectTheme(savedTheme);
      } else {
        // First time visitor, show theme selector after a brief delay
        setTimeout(() => {
          document.body.classList.remove('loading');
        }, 500);
      }
    });

    // Video modal functions
    function openModal(modalId, videoUrl) {
      const modal = document.getElementById(modalId);
      const iframe = modal.querySelector('iframe');
      iframe.src = videoUrl;
      modal.style.display = 'block';
    }

    function closeModal(modalId) {
      const modal = document.getElementById(modalId);
      const iframe = modal.querySelector('iframe');
      iframe.src = '';
      modal.style.display = 'none';
    }

    // Close modal when clicking outside
    window.onclick = function(event) {
      if (event.target.classList.contains('video-modal')) {
        closeModal(event.target.id);
      }
    }
  </script>
  </div>
</body>

</html>
